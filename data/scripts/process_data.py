#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†è„šæœ¬
ç”¨äºæ‰¹é‡å¤„ç†æ ¡å›­å¯¼è§ˆæ•°æ®ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒ
"""

import asyncio
import logging
import sys
import json
from pathlib import Path
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.processing.pipeline import DataProcessingPipeline, create_pipeline
from config.settings import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, settings.log_level.upper()),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def process_data(
    text_dir: Optional[str] = None,
    image_dir: Optional[str] = None,
    metadata_file: Optional[str] = None,
    output_dir: str = "data/processed",
    batch_size: int = None
):
    """
    å¤„ç†æ•°æ®çš„ä¸»å‡½æ•°
    
    Args:
        text_dir: æ–‡æœ¬ç›®å½•è·¯å¾„
        image_dir: å›¾åƒç›®å½•è·¯å¾„
        metadata_file: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    # ä½¿ç”¨é…ç½®çš„æ‰¹å¤„ç†å¤§å°æˆ–é»˜è®¤å€¼
    if batch_size is None:
        batch_size = settings.batch_size
    
    print("="*60)
    print("å¤šæ¨¡æ€RAGç³»ç»Ÿ - æ•°æ®å¤„ç†")
    print("="*60)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if text_dir:
        text_path = Path(text_dir)
        if not text_path.exists():
            print(f"âŒ æ–‡æœ¬ç›®å½•ä¸å­˜åœ¨: {text_dir}")
            return False
        print(f"ğŸ“ æ–‡æœ¬ç›®å½•: {text_dir}")
    
    if image_dir:
        image_path = Path(image_dir)
        if not image_path.exists():
            print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {image_dir}")
            return False
        print(f"ğŸ–¼ï¸  å›¾åƒç›®å½•: {image_dir}")
    
    if metadata_file:
        metadata_path = Path(metadata_file)
        if not metadata_path.exists():
            print(f"âš ï¸  å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            metadata_file = None
        else:
            print(f"ğŸ“„ å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
    
    print(f"âš™ï¸  æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print("-"*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    try:
        # åˆ›å»ºæ•°æ®å¤„ç†ç®¡é“
        print("ğŸ”„ åˆå§‹åŒ–æ•°æ®å¤„ç†ç®¡é“...")
        pipeline = await create_pipeline()
        
        # å¤„ç†æ•°æ®
        print("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®...")
        stats = await pipeline.process_directory(
            text_dir=text_dir,
            image_dir=image_dir,
            metadata_file=metadata_file
        )
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*60)
        print("æ•°æ®å¤„ç†å®Œæˆ")
        print("="*60)
        pipeline.print_stats()
        
        # ä¿å­˜å¤„ç†ç»Ÿè®¡
        stats_file = output_path / "processing_stats.json"
        stats_data = {
            "text_files_processed": stats.processed_texts,
            "text_files_failed": stats.failed_texts,
            "text_files_total": stats.total_texts,
            "image_files_processed": stats.processed_images,
            "image_files_failed": stats.failed_images,
            "image_files_total": stats.total_images,
            "relations_created": stats.created_relations,
            "elapsed_time_seconds": stats.elapsed_time,
            "text_success_rate": stats.text_success_rate,
            "image_success_rate": stats.image_success_rate,
            "config": {
                "batch_size": batch_size,
                "text_dir": text_dir,
                "image_dir": image_dir,
                "metadata_file": metadata_file
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        log_file = output_path / "processing_log.txt"
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®å¤„ç†æ—¥å¿—\n")
            f.write(f"æ—¶é—´: {stats_data.get('timestamp', 'N/A')}\n")
            f.write(f"æ–‡æœ¬æ–‡ä»¶: {stats.processed_texts}/{stats.total_texts} "
                   f"({stats.text_success_rate*100:.1f}%)\n")
            f.write(f"å›¾åƒæ–‡ä»¶: {stats.processed_images}/{stats.total_images} "
                   f"({stats.image_success_rate*100:.1f}%)\n")
            f.write(f"å…³è”åˆ›å»º: {stats.created_relations}\n")
            f.write(f"æ€»è€—æ—¶: {stats.elapsed_time:.2f}ç§’\n")
        
        print(f"ğŸ“ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return False


def find_data_directories():
    """æŸ¥æ‰¾æ•°æ®ç›®å½•"""
    base_dir = Path("data/raw")
    
    text_dir = None
    image_dir = None
    metadata_file = None
    
    if (base_dir / "text").exists():
        text_dir = str(base_dir / "text")
    elif (base_dir / "texts").exists():
        text_dir = str(base_dir / "texts")
    
    if (base_dir / "images").exists():
        image_dir = str(base_dir / "images")
    elif (base_dir / "image").exists():
        image_dir = str(base_dir / "image")
    
    if (base_dir / "metadata.json").exists():
        metadata_file = str(base_dir / "metadata.json")
    
    return text_dir, image_dir, metadata_file


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šæ¨¡æ€RAGç³»ç»Ÿæ•°æ®å¤„ç†è„šæœ¬")
    parser.add_argument("--text-dir", help="æ–‡æœ¬ç›®å½•è·¯å¾„")
    parser.add_argument("--image-dir", help="å›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--metadata", help="å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", default="data/processed", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--batch-size", type=int, help="æ‰¹å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®å½•ï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾
    text_dir = args.text_dir
    image_dir = args.image_dir
    metadata_file = args.metadata
    
    if not text_dir and not image_dir:
        print("ğŸ” æœªæŒ‡å®šæ•°æ®ç›®å½•ï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾...")
        found_text, found_image, found_metadata = find_data_directories()
        
        if found_text:
            text_dir = found_text
            print(f"  æ‰¾åˆ°æ–‡æœ¬ç›®å½•: {text_dir}")
        
        if found_image:
            image_dir = found_image
            print(f"  æ‰¾åˆ°å›¾åƒç›®å½•: {image_dir}")
        
        if found_metadata:
            metadata_file = found_metadata
            print(f"  æ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
        
        if not text_dir and not image_dir:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®ç›®å½•ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  python scripts/process_data.py --text-dir /path/to/texts --image-dir /path/to/images")
            print("  æˆ–")
            print("  å°†æ•°æ®æ”¾å…¥ data/raw/texts/ å’Œ data/raw/images/ ç›®å½•")
            return 1
    
    # è¿è¡Œæ•°æ®å¤„ç†
    success = asyncio.run(
        process_data(
            text_dir=text_dir,
            image_dir=image_dir,
            metadata_file=metadata_file,
            output_dir=args.output_dir,
            batch_size=args.batch_size
        )
    )
    
    if success:
        print("\n" + "="*60)
        print("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥å»ºè®®ï¼š")
        print("1. å¯åŠ¨APIæœåŠ¡å™¨: python -m src.api.main")
        print("2. è¿è¡Œæµ‹è¯•: python scripts/test_retrieval.py")
        print("3. è®¿é—® http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£")
        print("="*60)
        return 0
    else:
        print("\næ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())